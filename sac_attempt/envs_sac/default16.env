chosen_path_learning=./objects_sac/default16/learning
chosen_path_saving=./objects_sac/default16/saving
chosen_path_log_loss=./objects_sac/default16/logs/log_loss.txt
chosen_path_log_eval=./objects_sac/default16/logs/log_eval.txt
value_function_path=./objects_sac/default16/value_functions
optimal_action_path=./objects_sac/default16/optimal_actions
# ############################################################
reuse_weights=0
# 1 for Yes, 0 for No
which_forecast=0
# ############################################################

start_time=1410
end_time=1440
number_of_steps=1
step_size=30

# Hyperparameters

initial_num_iterations=100000
sub_num_iterations=100000

initial_collect_steps=10000
collect_steps_per_iteration=1
replay_buffer_capacity=100000

batch_size=256

critic_learning_rate=1e-4
actor_learning_rate=1e-4
alpha_learning_rate=1e-4
target_update_tau=0.005
target_update_period=1
gamma=1
reward_scale_factor=1.0

actor_fc_layer_params=16,16
critic_joint_fc_layer_params=16,16

log_interval=1000

num_eval_episodes=20
eval_interval=10000

policy_save_interval=5000


# ############################################################
# Battery info
C_esM=4.8
C_esm=0.96

C_tsM=3.5
C_tsm=0

P_esm=0.75
P_esM=-0.85
T_esm=2.8
T_esM=-5

eta_WH=0.95
eta_PV=0.10
eta_C_es=1.156
eta_D_es=0.88
eta_C_ts=1.040
eta_D_ts=0.961

D_es=11
D_ts=24

D_pes=3.8194e-05
D_pts=1.1429e-04

F_SD=1e-06
